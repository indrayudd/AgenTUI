import { ChatOpenAI } from '@langchain/openai';
import { createDeepAgent, FilesystemBackend } from 'deepagents';
import type { BaseMessage } from '@langchain/core/messages';
import type { AppConfig } from '../config/index.js';
import type { UsageMetadata } from '../state/usage.js';
import { buildTools } from '../tools/index.js';

const parsedLimit = Number(process.env.AGEN_TUI_RECURSION_LIMIT ?? '100');
const DEFAULT_RECURSION_LIMIT = Number.isFinite(parsedLimit) && parsedLimit > 0 ? parsedLimit : 100;

export type AgentRole = 'user' | 'assistant' | 'system';
export interface AgentMessage {
  role: AgentRole;
  content: string;
}

export interface AgentResult {
  text: string;
  finalMessage?: BaseMessage;
  usage?: UsageMetadata | null;
  model?: string;
}

export interface AgentRunner {
  run: (messages: AgentMessage[]) => Promise<AgentResult>;
  stream: (
    messages: AgentMessage[],
    options?: Parameters<ReturnType<typeof createDeepAgent>['streamEvents']>[2]
  ) => AsyncIterable<any>;
}

export const createAgentRunner = (config: AppConfig): AgentRunner => {
  const model = new ChatOpenAI({
    apiKey: config.openAIApiKey,
    model: config.openAIModel
  });

  const workspaceRoot = process.cwd();
  const backend = new FilesystemBackend({
    rootDir: workspaceRoot,
    virtualMode: false
  });
  const customTools = buildTools(workspaceRoot);

  const agent = createDeepAgent({
    model,
    systemPrompt: config.systemPrompt,
    backend,
    tools: customTools
  });

  const run = async (messages: AgentMessage[]): Promise<AgentResult> => {
    const result = await agent.invoke({ messages }, { recursionLimit: DEFAULT_RECURSION_LIMIT });
    const finalMessage = result.messages[result.messages.length - 1];
    let text = '';
    const content = finalMessage?.content;
    if (typeof content === 'string') {
      text = content;
    } else if (Array.isArray(content)) {
      text = content
        .map((part) => {
          if (typeof part === 'string') return part;
          if (typeof part === 'object' && part !== null && 'text' in part) {
            const maybeText = (part as { text?: string }).text;
            return maybeText ?? '';
          }
          return JSON.stringify(part);
        })
        .join('\n');
    } else if (content && typeof content === 'object') {
      text = JSON.stringify(content);
    }

    const usage = extractUsage(finalMessage);
    const actualModel = extractModelIdentifier(finalMessage);
    return { text, finalMessage, usage, model: actualModel };
  };

  const stream = (
    messages: AgentMessage[],
    options?: Parameters<typeof agent.streamEvents>[2]
  ) => agent.streamEvents({ messages }, { recursionLimit: DEFAULT_RECURSION_LIMIT }, options);

  return { run, stream };
};

const extractModelIdentifier = (message?: BaseMessage | null): string | undefined => {
  if (!message) return undefined;
  const metadata = message.response_metadata as
    | {
        model?: string;
        model_name?: string;
        response?: { model?: string };
      }
    | undefined;
  return metadata?.model ?? metadata?.model_name ?? metadata?.response?.model;
};

const extractUsage = (message?: BaseMessage | null): UsageMetadata | null => {
  if (!message) return null;
  const maybeUsage = (message as BaseMessage & { usage_metadata?: UsageMetadata }).usage_metadata;
  if (maybeUsage) return maybeUsage;
  const metadata = message.response_metadata as
    | { tokenUsage?: { promptTokens?: number; completionTokens?: number; totalTokens?: number } }
    | undefined;
  const tokenUsage = metadata?.tokenUsage;
  if (!tokenUsage) return null;
  return {
    input_tokens: tokenUsage.promptTokens ?? undefined,
    output_tokens: tokenUsage.completionTokens ?? undefined,
    total_tokens: tokenUsage.totalTokens ?? undefined
  };
};
